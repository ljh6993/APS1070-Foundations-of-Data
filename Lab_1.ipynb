{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBvzhAlwoJFp"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "# APS1070\n",
    "#### Basic Principles and Models - Project 1\n",
    "from jianhui li;    \n",
    "student number: 1002116907\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJqa8JCbsXAR"
   },
   "source": [
    "Project 1 has two parts: a tutorial component (which will be covered in labs) and an exercises component (to be completed as homework, individually). Overall, this project is worth 12.5% of your final grade. Completing the tutorial section is worth 2.5 marks. The exercises section will be graded out of the remaining 10 marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02vFf7HBoJFq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AC7fb3NUoJFq"
   },
   "source": [
    "In this first lab, we will be using the popular machine learning library [scikit-learn](https://scikit-learn.org/stable/) in tandem with a popular scientific computing library in Python, [NumPy](https://www.numpy.org/), to investigate basic machine learning principles and models. The topics that will be covered in this lab include:\n",
    "* Introduction to scikit-learn and NumPy\n",
    "* Data preparation and cleaning with Pandas\n",
    "* Exploratory data analysis (EDA)\n",
    "* Nearest neighbors classification algorithm\n",
    "\n",
    "*Note:* Some other useful Python libraries include [matplotlib](https://matplotlib.org/) (for plotting/graphing) and [Pandas](https://pandas.pydata.org/) (for data analysis), though we won't be going into detail on these in this bootcamp. \n",
    "\n",
    "##### Jupyter Notebooks\n",
    "This lab will be using [Jupyter Notebooks](https://jupyter.org/) as a Python development environment. Hopefully you're somewhat familiar with them. Write your code in *cells* (this is a cell!) and execute your code by pressing the *play* button (up top) or by entering *ctrl+enter*. To format a cell for text, you can select \"Markdown\" from the dropdown - the default formatting is \"Code\", which will usually be what you want.\n",
    "\n",
    "#### Getting started\n",
    "Let's get started. First, we're going to test that we're able to import the required libraries.  \n",
    "**>> Run the code in the next cell** to import scikit-learn and NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXYj_1fHoJFs"
   },
   "source": [
    "### NumPy Basics\n",
    "\n",
    "Great. Let's move on to our next topic: getting a handle on NumPy basics. You can think of NumPy as sort of like a MATLAB for Python (if that helps). The main object is multidimensional arrays, and these come in particularly handy when working with data and machine learning algorithms.\n",
    "\n",
    "Let's create a 2x4 array containing the numbers 1 through 8 and conduct some basic operations on it.  \n",
    "**>> Run the code in the next cell to create and print the array.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1569440238949,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "xs3V4laeoJFt",
    "outputId": "4ec13658-db99-4d2b-e56f-491e7bc299cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [4, 5, 6, 7]])"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.arange(8).reshape(2,4)\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bknAcISQ1LgF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xICCkhfNoJFu"
   },
   "source": [
    "We can access the shape, number of dimensions, data type, and number of elements in our array as follows:  \n",
    "*(Tip: use \"print()\" when you want a cell to output more than one thing, or you want to append text to your output, otherwise the cell will output the last object you call, as in the cell above)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1569440238949,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "ncoFbUrloJFv",
    "outputId": "e34613f7-2cfd-4d9b-9f76-e01cc0aaf754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2, 4)\n",
      "Dimensions: 2\n",
      "Data type: int64\n",
      "Number of elements: 8\n"
     ]
    }
   ],
   "source": [
    "print (\"Shape:\", array.shape)\n",
    "print (\"Dimensions:\", array.ndim)\n",
    "print (\"Data type:\" , array.dtype.name)\n",
    "print (\"Number of elements:\", array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_9lG_HUoJFw"
   },
   "source": [
    "If we have a Python list containing a set of numbers, we can use it to create an array:  \n",
    "*(Tip: if you click on a function call, such as array(), and press \"shift+tab\" the Notebook will provide you all the details of the function)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1569440238950,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "YBuAwh7ZoJFw",
    "outputId": "3a3d3367-6fed-49db-c13a-612ba7eba293"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 5],\n",
       "       [ 8],\n",
       "       [13],\n",
       "       [21]])"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = [0, 1, 1, 2, 3, 5, 8, 13, 21]\n",
    "myarray = np.array(mylist)\n",
    "myarray.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VuaWRVPzoJFy"
   },
   "source": [
    "And we can do it for nested lists as well, creating multidimensional NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 757,
     "status": "ok",
     "timestamp": 1569440238950,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "phcNKMipoJFy",
    "outputId": "221bf691-5b0f-4040-b551-836ac8aa0a8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my2dlist = [[1,2,3],[4,5,6]]\n",
    "my2darray = np.array(my2dlist)\n",
    "my2darray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eg4tCbtDoJF0"
   },
   "source": [
    "We can also index and slice NumPy arrays like we would do with a Python list or another container object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1569440238950,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "5Lud5Oz6oJF0",
    "outputId": "d187fc6a-ac71-454d-93a4-6dd77be03fb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originally:  [0 1 2 3 4 5 6 7 8 9]\n",
      "First four elements:  [0 1 2 3]\n",
      "After the first four elements:  [4 5 6 7 8 9]\n",
      "The last element:  9\n"
     ]
    }
   ],
   "source": [
    "array = np.arange(10)\n",
    "print (\"Originally: \", array)\n",
    "print (\"First four elements: \", array[:4])\n",
    "print (\"After the first four elements: \", array[4:])\n",
    "print (\"The last element: \", array[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jk9CDc_moJF2"
   },
   "source": [
    "And we can index/slice multidimensional arrays, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 745,
     "status": "ok",
     "timestamp": 1569440238951,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "L4gkgltBoJF2",
    "outputId": "6ef2eba9-60a5-4243-cd5c-9214905f312c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originally:  [[1 2 3]\n",
      " [4 5 6]]\n",
      "First row only:  [1 2 3]\n",
      "First column only:  [1 4]\n"
     ]
    }
   ],
   "source": [
    "array = np.array([[1,2,3],[4,5,6]])\n",
    "print (\"Originally: \", array)\n",
    "print (\"First row only: \", array[0])\n",
    "print (\"First column only: \", array[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kimFdIJoJF5"
   },
   "source": [
    "#### Sneak preview\n",
    "\n",
    "Often, when designing a machine learning classifier, it can be useful to compare an array of predictions (0 or 1 values) to another array of true values. We can do this pretty easily in NumPy to compute the *accuracy* (e.g., the number of values that are the same), for example, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 738,
     "status": "ok",
     "timestamp": 1569440238951,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "UrVyjSiYoJF6",
    "outputId": "7f97979f-7e44-4410-a5e1-58a964ddc684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  70.0 %\n"
     ]
    }
   ],
   "source": [
    "true_values = [0, 0, 1, 1, 1, 1, 1, 0, 1, 0]\n",
    "predictions = [0, 0, 0, 1, 1, 1, 0, 1, 1, 0]\n",
    "\n",
    "true_values_array = np.array(true_values)\n",
    "predictions_array = np.array(predictions)\n",
    "\n",
    "accuracy = np.sum(true_values_array == predictions_array) / true_values_array.size\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTrKw3aEoJF7"
   },
   "source": [
    "In the previous cell, we took two Python lists, converted them to NumPy arrays, and then used a combination of np.sum() and .size to compute the accuracy (proportion of elements that are pairwise equal). A tiny bit more advanced, but demonstrates the power of NumPy arrays.\n",
    "\n",
    "You'll notice we didn't used nested loops to conduct the comparison, but instead used the np.sum() function. This is an example of a vectorized operation within NumPy that is much more efficient when dealing with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZzw94WdoJF8"
   },
   "source": [
    "### Pandas basics\n",
    "\n",
    "Pandas is an incredibly useful library that allows us to work with large datasets in Python. It contains myriad useful tools, and is highly compatible with other libraries like Scikit-learn, so you don't have to spend any time getting the two to play nicely together.\n",
    "\n",
    "First we are going to load a dataset with Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3085,
     "status": "ok",
     "timestamp": 1569440241307,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "Z8Nr62k7oJF8",
    "outputId": "2ef35018-69be-425b-8ef9-53e298ec5e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jianhuili/Library/Caches/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3665,
     "status": "ok",
     "timestamp": 1569440241892,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "TFs45of3oJF9",
    "outputId": "cedef249-3555-4d03-8f73-dcc2ba8af140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arabica_data.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "wget.download(\n",
    "    'https://github.com/alexwolson/APS1070_data/raw/master/arabica_data.csv',\n",
    "    'arabica_data.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PrgXCIXeoJF_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('arabica_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BNyU-L7hoJGA"
   },
   "source": [
    "With Pandas, the main object we work with is referred to as a _DataFrame_ (hence calling our object here df). A DataFrame stores our dataset in a way that immediately gives us a lot of power to interact with it. If you just put the DataFrame in a cell on its own, you instantly get a clear, easy to read preview of the data you have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mwyULfgooJGB"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRMk05QJoJGC"
   },
   "source": [
    "But even though this is printed out well, the dataset is a bit too large for this view to be anything but overwhelming. Luckily, Pandas allows us to easily get some summary statistics about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IdcIgH0roJGD",
    "outputId": "30553b68-dda6-4926-fc13-531e436c506e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Acidity</th>\n",
       "      <th>Aftertaste</th>\n",
       "      <th>Aroma</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Body</th>\n",
       "      <th>Category.One.Defects</th>\n",
       "      <th>Category.Two.Defects</th>\n",
       "      <th>Clean Cup</th>\n",
       "      <th>Cupper Points</th>\n",
       "      <th>Flavor</th>\n",
       "      <th>Moisture</th>\n",
       "      <th>Number of Bags</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>Uniformity</th>\n",
       "      <th>altitude_high_meters</th>\n",
       "      <th>altitude_low_meters</th>\n",
       "      <th>altitude_mean_meters</th>\n",
       "      <th>quality_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.00000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>1311.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>655.000000</td>\n",
       "      <td>7.538764</td>\n",
       "      <td>7.403158</td>\n",
       "      <td>7.569527</td>\n",
       "      <td>7.523288</td>\n",
       "      <td>7.523387</td>\n",
       "      <td>0.450038</td>\n",
       "      <td>3.626240</td>\n",
       "      <td>9.83312</td>\n",
       "      <td>7.502441</td>\n",
       "      <td>7.523539</td>\n",
       "      <td>0.088963</td>\n",
       "      <td>153.678108</td>\n",
       "      <td>9.910900</td>\n",
       "      <td>9.839497</td>\n",
       "      <td>1808.751552</td>\n",
       "      <td>1759.456703</td>\n",
       "      <td>1784.104128</td>\n",
       "      <td>82.148825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>378.597412</td>\n",
       "      <td>0.319773</td>\n",
       "      <td>0.349945</td>\n",
       "      <td>0.315930</td>\n",
       "      <td>0.349174</td>\n",
       "      <td>0.293089</td>\n",
       "      <td>2.017571</td>\n",
       "      <td>5.482857</td>\n",
       "      <td>0.77135</td>\n",
       "      <td>0.428989</td>\n",
       "      <td>0.341817</td>\n",
       "      <td>0.047907</td>\n",
       "      <td>129.760079</td>\n",
       "      <td>0.454824</td>\n",
       "      <td>0.491508</td>\n",
       "      <td>8767.192330</td>\n",
       "      <td>8767.851565</td>\n",
       "      <td>8767.021485</td>\n",
       "      <td>2.893505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>6.170000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>6.080000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.170000</td>\n",
       "      <td>6.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>43.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>327.500000</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>7.420000</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>81.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>655.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>7.420000</td>\n",
       "      <td>7.580000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>7.580000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1350.000000</td>\n",
       "      <td>1310.640000</td>\n",
       "      <td>1310.640000</td>\n",
       "      <td>82.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>982.500000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>7.580000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>7.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1650.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>83.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1310.000000</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>8.670000</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>8.580000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.830000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>1062.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>190164.000000</td>\n",
       "      <td>190164.000000</td>\n",
       "      <td>190164.000000</td>\n",
       "      <td>90.580000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      Acidity   Aftertaste        Aroma      Balance  \\\n",
       "count  1311.000000  1311.000000  1311.000000  1311.000000  1311.000000   \n",
       "mean    655.000000     7.538764     7.403158     7.569527     7.523288   \n",
       "std     378.597412     0.319773     0.349945     0.315930     0.349174   \n",
       "min       0.000000     5.250000     6.170000     5.080000     6.080000   \n",
       "25%     327.500000     7.330000     7.250000     7.420000     7.330000   \n",
       "50%     655.000000     7.500000     7.420000     7.580000     7.500000   \n",
       "75%     982.500000     7.750000     7.580000     7.750000     7.750000   \n",
       "max    1310.000000     8.750000     8.670000     8.750000     8.750000   \n",
       "\n",
       "              Body  Category.One.Defects  Category.Two.Defects   Clean Cup  \\\n",
       "count  1311.000000           1311.000000           1311.000000  1311.00000   \n",
       "mean      7.523387              0.450038              3.626240     9.83312   \n",
       "std       0.293089              2.017571              5.482857     0.77135   \n",
       "min       5.250000              0.000000              0.000000     0.00000   \n",
       "25%       7.330000              0.000000              0.000000    10.00000   \n",
       "50%       7.500000              0.000000              2.000000    10.00000   \n",
       "75%       7.670000              0.000000              4.000000    10.00000   \n",
       "max       8.580000             31.000000             55.000000    10.00000   \n",
       "\n",
       "       Cupper Points       Flavor     Moisture  Number of Bags    Sweetness  \\\n",
       "count    1311.000000  1311.000000  1311.000000     1311.000000  1311.000000   \n",
       "mean        7.502441     7.523539     0.088963      153.678108     9.910900   \n",
       "std         0.428989     0.341817     0.047907      129.760079     0.454824   \n",
       "min         5.170000     6.080000     0.000000        0.000000     1.330000   \n",
       "25%         7.250000     7.330000     0.090000       14.000000    10.000000   \n",
       "50%         7.500000     7.580000     0.110000      170.000000    10.000000   \n",
       "75%         7.750000     7.750000     0.120000      275.000000    10.000000   \n",
       "max        10.000000     8.830000     0.280000     1062.000000    10.000000   \n",
       "\n",
       "        Uniformity  altitude_high_meters  altitude_low_meters  \\\n",
       "count  1311.000000           1084.000000          1084.000000   \n",
       "mean      9.839497           1808.751552          1759.456703   \n",
       "std       0.491508           8767.192330          8767.851565   \n",
       "min       6.000000              1.000000             1.000000   \n",
       "25%      10.000000           1100.000000          1100.000000   \n",
       "50%      10.000000           1350.000000          1310.640000   \n",
       "75%      10.000000           1650.000000          1600.000000   \n",
       "max      10.000000         190164.000000        190164.000000   \n",
       "\n",
       "       altitude_mean_meters  quality_score  \n",
       "count           1084.000000    1311.000000  \n",
       "mean            1784.104128      82.148825  \n",
       "std             8767.021485       2.893505  \n",
       "min                1.000000      43.130000  \n",
       "25%             1100.000000      81.170000  \n",
       "50%             1310.640000      82.500000  \n",
       "75%             1600.000000      83.670000  \n",
       "max           190164.000000      90.580000  "
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eu_NKdsUoJGE"
   },
   "source": [
    "Let's say we want to zero in on a single column. This is done the same way that you access a dictionary entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AfhzRXIoJGE"
   },
   "outputs": [],
   "source": [
    "df['Species'] \n",
    "#speics is one of the attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PL32j4QcoJGF"
   },
   "source": [
    "Using this method of column access on its own returns a `series` object - think of this as a DataFrame with only one column. **If you want to get the raw values however**, you can simply specify this by adding `.values` after your entry. Using this, and by putting the object in a `Set` (which does not allow duplicate entries), we can quickly see all of the possible values for any column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1183,
     "status": "error",
     "timestamp": 1570591186563,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "ZQLTkn5LoJGG",
    "outputId": "5812e015-b630-47f9-8f67-6674199d20a0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ce143c295187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Variety'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "set(df['Variety'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6fJoOBgjoJGI"
   },
   "source": [
    "You may notice that the final entry in this set isn't like the others - it's `nan`, which in Pandas denotes a missing entry. When working with real world datasets it's very common for entries to be missing, and there are a variety of ways of approaching a problem like this. For now, though, we are simply going to tell Pandas to **drop any row that has a missing column**, using the `dropna()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vy3se4KxoJGI",
    "outputId": "f09cc875-b20a-47fa-825e-5e22bbd7a2e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arusha',\n",
       " 'Bourbon',\n",
       " 'Catimor',\n",
       " 'Catuai',\n",
       " 'Caturra',\n",
       " 'Gesha',\n",
       " 'Mandheling',\n",
       " 'Mundo Novo',\n",
       " 'Other',\n",
       " 'Pacamara',\n",
       " 'Pacas',\n",
       " 'Peaberry',\n",
       " 'Ruiru 11',\n",
       " 'SL14',\n",
       " 'SL28',\n",
       " 'SL34',\n",
       " 'Sumatra',\n",
       " 'Typica',\n",
       " 'Yellow Bourbon'}"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df.dropna()\n",
    "set(df_clean['Variety'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JdkOZCJloJGK"
   },
   "source": [
    "**YOUR TURN** \n",
    "\n",
    "*How many entries did we lose by dropping all `nan`s?\n",
    "* What percentage of entries are left in `df_clean`? ______\n",
    "* What column had the highest number of `nan` entries? (This can be done in one line - use Google!) ______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4061,
     "status": "ok",
     "timestamp": 1569440242332,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "1HaVQYqjoJGK",
    "outputId": "c9007838-355a-4183-cba9-7f682843c9c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26950 entries we lose by dropping all nans\n",
      "41.27 percentage of entries are left in df_clean\n",
      "What column had the highest number of nan entries: Farm Name\n"
     ]
    }
   ],
   "source": [
    "### Your code here\n",
    "#1)\n",
    "number=df.size-df_clean.size\n",
    "print(number,'entries we lose by dropping all nans')\n",
    "#2)\n",
    "#shape\n",
    "percentage=df_clean.size/df.size*100\n",
    "print('%0.2f' % percentage,'percentage of entries are left in df_clean')\n",
    "#3)\n",
    "#arr=np.matrix(df.isnull().sum().sort_values)\n",
    "#df.max(axis = 1) \n",
    "## answer: farm name have the highest number 356\n",
    "df.count().idxmin() \n",
    "print('What column had the highest number of nan entries:', df.count().idxmin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1p4DU9poJGN"
   },
   "source": [
    "As you perform this analysis, you will probably notice that we've lost _quite a bit_ of our original data by simply dropping the `nan` values. There is another approach that we can examine, however. Instead of dropping the missing entries entirely, we can _impute_ their value using the data we do have. For a single column we can do this like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovWpslp6Ka6T"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9F6N5-_uoJGN"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(\n",
    "    missing_values=np.nan,\n",
    "    strategy='mean',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "imp.fit(\n",
    "    df['altitude_mean_meters'].values.reshape((-1,1)) #we have to do the reshape operation because we are only using one feature.\n",
    ")\n",
    "\n",
    "df['altitude_mean_meters_imputed'] = imp.transform(df['altitude_mean_meters'].values.reshape((-1,1)))\n",
    "#reshape(-1,1), change into one column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4051,
     "status": "ok",
     "timestamp": 1569440242333,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "Mfp3D574oJGP",
    "outputId": "54a5a5ae-27f8-498a-d49f-e4517b718c2a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>altitude_mean_meters</th>\n",
       "      <th>altitude_mean_meters_imputed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2075.0</td>\n",
       "      <td>2075.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2075.0</td>\n",
       "      <td>2075.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1700.0</td>\n",
       "      <td>1700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2075.0</td>\n",
       "      <td>2075.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1784.104128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1784.104128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1635.0</td>\n",
       "      <td>1635.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1635.0</td>\n",
       "      <td>1635.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1822.5</td>\n",
       "      <td>1822.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   altitude_mean_meters  altitude_mean_meters_imputed\n",
       "0                2075.0                   2075.000000\n",
       "1                2075.0                   2075.000000\n",
       "2                1700.0                   1700.000000\n",
       "3                2000.0                   2000.000000\n",
       "4                2075.0                   2075.000000\n",
       "5                   NaN                   1784.104128\n",
       "6                   NaN                   1784.104128\n",
       "7                1635.0                   1635.000000\n",
       "8                1635.0                   1635.000000\n",
       "9                1822.5                   1822.500000"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['altitude_mean_meters','altitude_mean_meters_imputed']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14A5mPi7oJGQ"
   },
   "source": [
    "OK, great! Now we have replaced the useless NaN values with the average height. While this obviously isn't as good as original data, in a lot of situations this can be a step up from losing rows entirely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DK7eW8caoJGR"
   },
   "source": [
    "Sophisticated analysis can be done in only a few lines using Pandas. Let's say that we want to get the average coffee rating by country. First, we can use the `groupby` method to automatically collect the results by country. Then, we can select the column we want - `quality_score` - and calculate its mean the same way we would using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xx0KTeIxoJGR",
    "outputId": "5a54910b-e56d-45f4-c627-921da2fdf9e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country of Origin\n",
       "Brazil                          82.330725\n",
       "China                           80.868000\n",
       "Colombia                        82.932000\n",
       "Costa Rica                      83.090000\n",
       "El Salvador                     82.804545\n",
       "Ethiopia                        87.792500\n",
       "Guatemala                       81.957832\n",
       "Haiti                           80.750000\n",
       "Honduras                        81.010476\n",
       "Indonesia                       81.524286\n",
       "Kenya                           85.415000\n",
       "Laos                            82.000000\n",
       "Malawi                          81.711818\n",
       "Mexico                          80.246087\n",
       "Myanmar                         80.666667\n",
       "Nicaragua                       79.333000\n",
       "Panama                          81.750000\n",
       "Peru                            77.000000\n",
       "Philippines                     80.312500\n",
       "Taiwan                          82.462895\n",
       "Tanzania, United Republic Of    82.411724\n",
       "Uganda                          83.778333\n",
       "Name: quality_score, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.groupby('Country of Origin')['quality_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jzvh5MU-oJGS"
   },
   "source": [
    "This is certainly interesting, but it could be presented better. First, all of the ratings are pretty high (what's the highest and lowest rating?). Let's standardize to unit mean and variance so that we can tell the difference more easily. We'll just do that on our subset here for now, but you can apply it to the entire dataset too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CBUOQRA9oJGS",
    "outputId": "aa5071cd-6d2b-4712-b464-2912e7a6c5da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country of Origin\n",
       "Brazil                          0.194625\n",
       "China                          -0.491541\n",
       "Colombia                        0.476684\n",
       "Costa Rica                      0.550802\n",
       "El Salvador                     0.416895\n",
       "Ethiopia                        2.756749\n",
       "Guatemala                       0.019701\n",
       "Haiti                          -0.546895\n",
       "Honduras                       -0.424705\n",
       "Indonesia                      -0.183677\n",
       "Kenya                           1.641462\n",
       "Laos                            0.039482\n",
       "Malawi                         -0.095705\n",
       "Mexico                         -0.783281\n",
       "Myanmar                        -0.585987\n",
       "Nicaragua                      -1.211611\n",
       "Panama                         -0.077794\n",
       "Peru                           -2.306024\n",
       "Philippines                    -0.752127\n",
       "Taiwan                          0.256626\n",
       "Tanzania, United Republic Of    0.232622\n",
       "Uganda                          0.873700\n",
       "Name: quality_score, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_means = df_clean.groupby('Country of Origin')['quality_score'].mean()\n",
    "mu,si = country_means.mean(), country_means.std() #Calculate the overall mean and standard deviation of the quality scores\n",
    "country_means -= mu #Subtract the mean from every entry\n",
    "country_means /= si #Divide every entry by the standard deviation\n",
    "country_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2dFpdBvuoJGU"
   },
   "source": [
    "This is a lot clearer! Finally, let's sort this list so that it's easier to compare entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ij1F28KzoJGU",
    "outputId": "0114ea9e-c604-4617-d00d-42485b4c1675"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country of Origin\n",
       "Peru                           -2.306024\n",
       "Nicaragua                      -1.211611\n",
       "Mexico                         -0.783281\n",
       "Philippines                    -0.752127\n",
       "Myanmar                        -0.585987\n",
       "Haiti                          -0.546895\n",
       "China                          -0.491541\n",
       "Honduras                       -0.424705\n",
       "Indonesia                      -0.183677\n",
       "Malawi                         -0.095705\n",
       "Panama                         -0.077794\n",
       "Guatemala                       0.019701\n",
       "Laos                            0.039482\n",
       "Brazil                          0.194625\n",
       "Tanzania, United Republic Of    0.232622\n",
       "Taiwan                          0.256626\n",
       "El Salvador                     0.416895\n",
       "Colombia                        0.476684\n",
       "Costa Rica                      0.550802\n",
       "Uganda                          0.873700\n",
       "Kenya                           1.641462\n",
       "Ethiopia                        2.756749\n",
       "Name: quality_score, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_means.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NoMBMddpoJGW"
   },
   "source": [
    "Finally, we'll look at indexing using Pandas. Let's say that we want to only look at the coffee entries from Taiwan. We can use the following syntax to identify those rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rft2VN4soJGW"
   },
   "outputs": [],
   "source": [
    "df_clean[df_clean['Country of Origin'] == 'Taiwan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pdy7AhiboJGY"
   },
   "source": [
    "Say that out of the Taiwanese coffees, we only want to look at those which are the Bourbon variety. We can also chain those indexing operations like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fK2anT_loJGY"
   },
   "outputs": [],
   "source": [
    "df_clean[df_clean['Country of Origin'] == 'Taiwan'][df_clean['Variety'] == 'Bourbon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JghslLyIoJGZ"
   },
   "source": [
    "### Scikit-learn Basics\n",
    "\n",
    "Scikit-learn is a great library to use for doing **machine learning** in Python. Data preparation, exploratory data analysis (EDA), classification, regression, clustering; it has it all. \n",
    "\n",
    "returns sample data as numpy arrays rather than a pandas data frame.\n",
    "\n",
    "Scikit-learn usually expects data to be in the form of a 2D matrix with dimensions *n_samples x n_features* with an additional column for the target. To get acquainted with scikit-learn, we are going to use the [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris), one of the most famous datasets in pattern recognition. \n",
    "\n",
    "Each entry in the dataset represents an iris plant, and is categorized as: \n",
    "\n",
    "* Setosa (class 0)\n",
    "* Versicolor (class 1)\n",
    "* Virginica (class 2)\n",
    "\n",
    "These represent the target classes to predict. Each entry also includes a set of features, namely:\n",
    "\n",
    "* Sepal width (cm)\n",
    "* Sepal length (cm)\n",
    "* Petal length (cm)\n",
    "* Petal width (cm)\n",
    "\n",
    "In the context of machine learning classification, the remainder of the lab is going to investigate the following question:  \n",
    "*Can we design a model that, based on the iris sample features, can accurately predict the iris sample class? *\n",
    "\n",
    "Scikit-learn has a copy of the iris dataset readily importable for us. Let's grab it now and conduct some EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "flfiIhwgoJGa"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()\n",
    "feature_data = iris_data.data\n",
    "iris_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4NUSHqyoJGb"
   },
   "source": [
    "**YOUR TURN:** \"feature_data\" now contains the feature data for all of the iris samples. \n",
    "* What is the shape of this feature data? ________________\n",
    "* The data type? ________________\n",
    "* How many samples are there? ________________\n",
    "* How many features are there? ________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1IHiamcoJGb",
    "outputId": "b6679a52-e8c7-4d1b-cb73-169c27e52a43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the shape of this feature data? (150, 4)\n",
      "the data type: float64\n",
      "How many samples are there?: 150\n",
      "How many features are there?: 4\n"
     ]
    }
   ],
   "source": [
    "## Enter your code here\n",
    "##What is the shape of this feature data? ____\n",
    "\n",
    "print('What is the shape of this feature data?',feature_data.shape )\n",
    "print('the data type:',feature_data.dtype.name)\n",
    "print('How many samples are there?:',feature_data.shape[0])\n",
    "print('How many features are there?:',feature_data.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQhxUGJFoJGc"
   },
   "source": [
    "Next, we will save the target classification data in a similar fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nYUsnObIoJGc"
   },
   "outputs": [],
   "source": [
    "target_data = iris_data.target\n",
    "target_names = iris_data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnqM5ucroJGd"
   },
   "source": [
    "**YOUR TURN:**\n",
    "* What values are in \"target_data\"? ________________\n",
    "* What is the data type? ________________\n",
    "* What values are in \"target_names\"? ________________\n",
    "* What is the data type? ____________\n",
    "* How many samples are of type \"setosa\"? ________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WFHlF8kEoJGe",
    "outputId": "e830f6d6-2e75-485a-c17b-9daec8820d06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What values are in \"target_data\"? ____\n",
      "{0, 1, 2}\n",
      "What is the data type? ____\n",
      "int64\n",
      "What values are in \"target_names\"? ____\n",
      "['setosa', 'versicolor', 'virginica']\n",
      "What is the data type? ____\n",
      "str320\n",
      "how many samples are of type\"setosa?\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "## Enter your code here\n",
    "print('What values are in \"target_data\"? ____',end='\\n')\n",
    "print(set(target_data))\n",
    "print('What is the data type? ____')\n",
    "print(target_data.dtype.name)\n",
    "print('What values are in \"target_names\"? ____')\n",
    "print(list(target_names))\n",
    "print('What is the data type? ____')\n",
    "print(target_names.dtype.name)\n",
    "print('how many samples are of type\"setosa?')\n",
    "setosa = feature_data[target_data==0]\n",
    "print(setosa.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VIjLD8RoJGf"
   },
   "source": [
    "We can also do some more visual EDA by plotting the samples according to a subset of the features and coloring the data points to coincide with the sample classification. We will use [matplotlib](https://matplotlib.org/), a powerful plotting library within Python, to accomplish this.\n",
    "\n",
    "For example, lets plot sepal width vs. sepal length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KM7zOqsqJBOS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60y6Yy7zoJGg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yxr-oTnaoJGh",
    "outputId": "92c41e6c-3c56-4233-d36c-700abf747246"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "setosa = feature_data[target_data==0]\n",
    "versicolor = feature_data[target_data==1]\n",
    "virginica = feature_data[target_data==2]\n",
    "\n",
    "plt.scatter(setosa[:,0], setosa[:,1], label=\"setosa\")\n",
    "plt.scatter(versicolor[:,0], versicolor[:,1], label=\"versicolor\")\n",
    "plt.scatter(virginica[:,0], virginica[:,1], label=\"virginica\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"sepal length (cm)\")\n",
    "plt.ylabel(\"sepal width (cm)\")\n",
    "plt.title(\"Visual EDA\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fMhm5xTwlbh"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqiWXVGToJGi"
   },
   "source": [
    "In the above step, we used boolean indexing to filter the feature data based on the target data class. This allowed us to create a scatter plot for each of the iris classes and distinguish them by color.\n",
    "\n",
    "*Observations*: We can see that the \"setosa\" class typically consists of medium-to-high sepal width with low-to-medium sepal length, while the other two classes have lower width and higher length. The \"virginica\" class appears to have the largest combination of the two. \n",
    "\n",
    "**YOUR TURN:** \n",
    "* Which of the iris classes is seperable based on sepal characteristics? ____setosa_\n",
    "* Which of the iris classes is not? __versicolor and viginica\n",
    "* Can we (easily) visualize each of the samples w.r.t. all features on the same plot? Why/why not? ____no! when all features on the same plot, the plot is no more a 2D plot, its hard to be visualize. this gragh only has two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6yar5YTAoJGj"
   },
   "source": [
    "### Creating a Nearest Neighbors Classifier\n",
    "\n",
    "Now that we've explored the data a little bit, we're going to use scikit-learn to create a nearest neighbors classifier for the data. Effectively we'll be developing a model whose job it is to build a relationship over input feature data (sepal and petal characteristics) that predicts the iris sample class (e.g. \"setosa\"). This is an example of a *supervised learning* task; we have all the features and all the target classes.\n",
    "\n",
    "Model creation in scikit-learn follows a **data prep -> fit -> predict** process. The \"fit\" function is where the actual model is trained and parameter values are selected, while the \"predict\" function actually takes the trained model and applies it to the new samples.\n",
    "\n",
    "First, we load the nearest neighbor library from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yFpq-h9oJGj"
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPzTtk8roJGk"
   },
   "source": [
    "Now, we're going to save our feature data into an array called 'X' and our target data into an array called 'y'. We don't *need* to do this, but it is traditional to think of the problem using this notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZX6WgMjoJGk"
   },
   "outputs": [],
   "source": [
    "X = feature_data\n",
    "y = target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sags3EJ4oJGm"
   },
   "source": [
    "Next, we create our nearest neighbor classifier object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TDby6Z-6oJGm"
   },
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ZaVns1ooJGo"
   },
   "source": [
    "And then we *fit* it to the data (i.e., train the classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4648,
     "status": "ok",
     "timestamp": 1569440243012,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "F2uDIsP9oJGq",
    "outputId": "8b12c935-80d9-45d0-e56b-a5693b330205"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9qLzshxoJGr"
   },
   "source": [
    "Now we have a model! If you're new to this, you've officially built your first machine learning model. If you use \"knn.predict(*[[feature array here]]*)\", you can use your trained model to predict the class of a new iris sample. \n",
    "\n",
    "**YOUR TURN:**\n",
    "* What is the predicted class of a new iris sample with feature vector [3,4,5,2]? What is its name? ________________virginica\n",
    "* Do you think this model is overfit or underfit to the iris dataset? Why? ________________overfit. because the nearest neighbor is one(k=1). when k=1, it gives too many details about the training data set, and cannot give a general model\n",
    "* How many neighbors does our model consider when classifying a new sample? ________________in our model, only one neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4643,
     "status": "ok",
     "timestamp": 1569440243012,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "ThmDOzVFoJGr",
    "outputId": "c82792e4-bfad-4504-c027-0ddbd53080c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####\n",
    "knn.predict([[3,4,5,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grjsglM5oJGs"
   },
   "source": [
    "As you may have noted in the previous cell, we've trained this classifier on our *entire dataset*. This typically isn't done in practice and results in overfitting to the data. Here's a bit of a tricky question:\n",
    "\n",
    "**YOUR TURN:**\n",
    "* If we use our classifier to predict the classes of the iris samples that were used to train the model itself, what will our overall accuracy be? ______accuracy should be 100%. because the classifier is summarized from the iris samples. \n",
    "\n",
    "We can validate our hypothesis fairly easily using either: i) the NumPy technique for calculating accuracy we used earlier in the lab, or ii) scikit-learn's in-house \"accuracy_score()\" function.\n",
    "\n",
    "Let's use our technique first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4638,
     "status": "ok",
     "timestamp": 1569440243012,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "czS77Os8oJGt",
    "outputId": "08c5f1be-a03b-488c-af4f-44d00b7ead31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.sum(target_data == knn.predict(feature_data)) / target_data.size\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlRFSBWIoJGu"
   },
   "source": [
    "and then using scikit-learn's customized function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4633,
     "status": "ok",
     "timestamp": 1569440243013,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "ae7QXH_EoJGu",
    "outputId": "82af00d5-fef1-4219-e808-8c63e2f0bc5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(target_data, knn.predict(feature_data))\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojZn8Ly8oJGw"
   },
   "source": [
    "We see that our classifier has achieved 100% accuracy (and both calculation methods agree)!\n",
    "\n",
    "**DISCUSSION:** \n",
    "###\n",
    "* Why do you think the model was able to achieve such a \"great\" result? \n",
    "answer The training data is equal to test data and the k=1. We use the training data to train the model, then we use the model to predict the train data. The result should be 100 %\n",
    "* What does this really tell us? \n",
    "So, we need to split the data into test data set and training data set, which could help us test the accuracy of the model.\n",
    "* Do you expect the model to perform this well on new data?\n",
    "No! The accuracy must be smaller than 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5K78afZoJG4"
   },
   "source": [
    "### Exercises (to be completed on your own)\n",
    "\n",
    "Let's take the tools we have learned in this lab and put them into practice on a new dataset.\n",
    "\n",
    "We're going to work with a dataset focused on diabetes. It contains a variety of health metrics for a number of patients, and then in a second object it shows whether or not that patient had diabetes. Download it using the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkFlVlNGoJG4"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import sklearn \n",
    "\n",
    "diabetes_data = fetch_openml(\n",
    "    name='diabetes',\n",
    "    cache=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pyzdTw7WEhH"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BLwmertOoJG5"
   },
   "source": [
    "First off, take a look at the `data`, `target` and `feature_names` entires in the `diabetes_data` dictionary. They contain the information we'll be working with here. Then, create a Pandas DataFrame called `diabetes_df` containing the data and the targets, with the feature names as column headings. If you need help, refer [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) for more detail on how to achieve this.\n",
    "\n",
    "* What was the average age of participants? [1] ____33.24 years\n",
    "* How many participants tested positive? How many tested negative? [1] ____268 positive 500 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7795,
     "status": "ok",
     "timestamp": 1569440246183,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "oOgVWHG-oJG5",
    "outputId": "8efffec1-12be-4641-ff3b-c8b37c47ccb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the average age of participants? 33.240885416666664\n",
      "How many participants tested positive?  268\n",
      "How many tested negative? 500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### YOUR CODE HERE\n",
    "featuredata=diabetes_data.data\n",
    "targetdata=diabetes_data.target\n",
    "featurenames=diabetes_data.feature_names\n",
    "#print(targetdata.shape)\n",
    "#d = {'data': targetdata, 'feature': featuredata}\n",
    "diabetes_df= pd.DataFrame(featuredata,columns=featurenames)\n",
    "diabetes_df['target']=targetdata\n",
    "meanage=diabetes_df['age'].mean()\n",
    "print('What was the average age of participants?',meanage)\n",
    "\n",
    "\n",
    "number_p = targetdata[targetdata=='tested_positive']\n",
    "\n",
    "number_n = targetdata[targetdata=='tested_negative']\n",
    "\n",
    "print('How many participants tested positive? ',number_p.size)\n",
    "print('How many tested negative?',number_n.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7qnyte6oJG6"
   },
   "source": [
    "The targets are currently a string representing whether or not the patient has diabetes. However, it's more useful for us if this column contains a 1 or a 0 depending on whether the patient has diabetes. Use the [Label Encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) class from Scikit-Learn to convert the labels into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqDw_D2doJG7"
   },
   "outputs": [],
   "source": [
    "#can only run for once\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(['tested_positive','tested_negative'])\n",
    "newarray=encoder.transform(diabetes_df['target'])\n",
    "diabetes_df['target']=newarray\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kANitNmToJG9"
   },
   "source": [
    "Now we are going to create a classifier to predict whether a patient has diabetes based on their vitals. \n",
    "\n",
    "Using `cross_val_score`, report mean cross validation accuracy on a KNN classifier with K=3 and 10 folds. Remember that the `target` column holds our labels.\n",
    "\n",
    "* What accuracy did the model achieve?[1] ____\n",
    "answer: 70.3%\n",
    "\n",
    "* Find a value for K that performs better than this. What value for K did you use? What was the performance? [2] ____\n",
    "answer:  the value for k is better to be 17, which give the highest cross value score 75.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9092,
     "status": "ok",
     "timestamp": 1569440247493,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "oIDm_Cn5oJG9",
    "outputId": "4727d380-3557-4a58-8c02-81cdcaea6352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7030587833219413\n",
      "0.7552973342447027\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "x = featuredata\n",
    "y = targetdata\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "#knn.fit(x,y)\n",
    "\n",
    "scores = cross_val_score(knn, x, y, cv=10, scoring='accuracy')\n",
    "print (scores.mean())\n",
    "\n",
    "k_range = range(1,30)\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, x, y, cv=10, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "print(max(k_scores))\n",
    "print(k_scores.index(max(k_scores))+1)\n",
    "\n",
    "# accuracy_0 = accuracy_score(targetdata, knn.predict(featuredata))\n",
    "# print (\"Accuracy: \", accuracy_0 * 100, \"%\")\n",
    "\n",
    "# knn = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
    "# knn.fit(x,y)\n",
    "# accuracy_1 = accuracy_score(targetdata, knn.predict(featuredata))\n",
    "# print (\"Accuracy: \", accuracy_1 * 100, \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYPM-3wHoJG-"
   },
   "source": [
    "Take a look at the `skin` feature.\n",
    "\n",
    "* According to the dataset description in `diabetes_data['DESCR']`, what does this feature represent? [1] ____\n",
    "answer: Triceps skin fold thickness (mm)\n",
    "* Are there any unusual entries in this column? If so, why? [2] ____\n",
    "these is some nan(zero) in the column skin. skin thickness should not be zero in the real world\n",
    "\n",
    "Use the `SimpleImputer` class from scikit-learn to impute missing values for the `skin` and `insu` columns. Overwrite the existing `skin` and `insu` columns with these new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90gCGSVHoJG-",
    "outputId": "505d6105-76bb-4248-c4f2-74f241f49b98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Author**: [Vincent Sigillito](vgs@aplcen.apl.jhu.edu)  \n",
      "\n",
      "**Source**: [Obtained from UCI](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes) \n",
      "\n",
      "**Please cite**: [UCI citation policy](https://archive.ics.uci.edu/ml/citation_policy.html)  \n",
      "\n",
      "1. Title: Pima Indians Diabetes Database\n",
      " \n",
      " 2. Sources:\n",
      "    (a) Original owners: National Institute of Diabetes and Digestive and\n",
      "                         Kidney Diseases\n",
      "    (b) Donor of database: Vincent Sigillito (vgs@aplcen.apl.jhu.edu)\n",
      "                           Research Center, RMI Group Leader\n",
      "                           Applied Physics Laboratory\n",
      "                           The Johns Hopkins University\n",
      "                           Johns Hopkins Road\n",
      "                           Laurel, MD 20707\n",
      "                           (301) 953-6231\n",
      "    (c) Date received: 9 May 1990\n",
      " \n",
      " 3. Past Usage:\n",
      "     1. Smith,~J.~W., Everhart,~J.~E., Dickson,~W.~C., Knowler,~W.~C., &\n",
      "        Johannes,~R.~S. (1988). Using the ADAP learning algorithm to forecast\n",
      "        the onset of diabetes mellitus.  In {it Proceedings of the Symposium\n",
      "        on Computer Applications and Medical Care} (pp. 261--265).  IEEE\n",
      "        Computer Society Press.\n",
      " \n",
      "        The diagnostic, binary-valued variable investigated is whether the\n",
      "        patient shows signs of diabetes according to World Health Organization\n",
      "        criteria (i.e., if the 2 hour post-load plasma glucose was at least \n",
      "        200 mg/dl at any survey  examination or if found during routine medical\n",
      "        care).   The population lives near Phoenix, Arizona, USA.\n",
      " \n",
      "        Results: Their ADAP algorithm makes a real-valued prediction between\n",
      "        0 and 1.  This was transformed into a binary decision using a cutoff of \n",
      "        0.448.  Using 576 training instances, the sensitivity and specificity\n",
      "        of their algorithm was 76% on the remaining 192 instances.\n",
      " \n",
      " 4. Relevant Information:\n",
      "       Several constraints were placed on the selection of these instances from\n",
      "       a larger database.  In particular, all patients here are females at\n",
      "       least 21 years old of Pima Indian heritage.  ADAP is an adaptive learning\n",
      "       routine that generates and executes digital analogs of perceptron-like\n",
      "       devices.  It is a unique algorithm; see the paper for details.\n",
      " \n",
      " 5. Number of Instances: 768\n",
      " \n",
      " 6. Number of Attributes: 8 plus class \n",
      " \n",
      " 7. For Each Attribute: (all numeric-valued)\n",
      "    1. Number of times pregnant\n",
      "    2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
      "    3. Diastolic blood pressure (mm Hg)\n",
      "    4. Triceps skin fold thickness (mm)\n",
      "    5. 2-Hour serum insulin (mu U/ml)\n",
      "    6. Body mass index (weight in kg/(height in m)^2)\n",
      "    7. Diabetes pedigree function\n",
      "    8. Age (years)\n",
      "    9. Class variable (0 or 1)\n",
      " \n",
      " 8. Missing Attribute Values: None\n",
      " \n",
      " 9. Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
      "    diabetes\")\n",
      " \n",
      "    Class Value  Number of instances\n",
      "    0            500\n",
      "    1            268\n",
      " \n",
      " 10. Brief statistical analysis:\n",
      " \n",
      "     Attribute number:    Mean:   Standard Deviation:\n",
      "     1.                     3.8     3.4\n",
      "     2.                   120.9    32.0\n",
      "     3.                    69.1    19.4\n",
      "     4.                    20.5    16.0\n",
      "     5.                    79.8   115.2\n",
      "     6.                    32.0     7.9\n",
      "     7.                     0.5     0.3\n",
      "     8.                    33.2    11.8\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Relabeled values in attribute 'class'\n",
      "    From: 0                       To: tested_negative     \n",
      "    From: 1                       To: tested_positive\n",
      "\n",
      "Downloaded from openml.org.\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "print(diabetes_data['DESCR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOVOxaE3Kr-m"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "impsi = SimpleImputer(\n",
    "    missing_values=0,\n",
    "    strategy='mean',\n",
    "    verbose=1\n",
    ")\n",
    "impsi.fit( diabetes_df['skin'].values.reshape((-1,1)) )\n",
    "diabetes_df['skin'] = impsi.transform(diabetes_df['skin'].values.reshape((-1,1)))\n",
    "\n",
    "impsi.fit( diabetes_df['insu'].values.reshape((-1,1)))\n",
    "diabetes_df['insu'] = impsi.transform(diabetes_df['insu'].values.reshape((-1,1)))\n",
    "\n",
    "# imp = SimpleImputer(\n",
    "#     missing_values=np.nan,\n",
    "#     strategy='mean',\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# imp.fit(\n",
    "#     df['altitude_mean_meters'].values.reshape((-1,1)) #we have to do the reshape operation because we are only using one feature.\n",
    "# )\n",
    "\n",
    "# df['altitude_mean_meters_imputed'] = imp.transform(df['altitude_mean_meters'].values.reshape((-1,1)))\n",
    "# #reshape(-1,1), change into one column \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRCZBbQ6MrEN",
    "outputId": "988ee864-49b8-4e14-e3a4-d69a1c191c86"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>32.00000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>45.00000</td>\n",
       "      <td>543.000000</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>19.00000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>47.00000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>38.00000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>41.00000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.704</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>35.4</td>\n",
       "      <td>0.388</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.451</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.263</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>33.00000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>31.1</td>\n",
       "      <td>0.205</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>39.4</td>\n",
       "      <td>0.257</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0.487</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>19.00000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>22.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>34.1</td>\n",
       "      <td>0.337</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>2.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17.00000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.453</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>1.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0.293</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>11.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>37.00000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>42.3</td>\n",
       "      <td>0.785</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>3.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>30.8</td>\n",
       "      <td>0.400</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>1.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>18.00000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.219</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>9.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>32.7</td>\n",
       "      <td>0.734</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>13.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>37.00000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1.174</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>12.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>33.00000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.488</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>1.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>41.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>49.3</td>\n",
       "      <td>0.358</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>41.00000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>46.3</td>\n",
       "      <td>1.096</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>3.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>22.00000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>36.4</td>\n",
       "      <td>0.408</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>6.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.178</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>4.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>31.2</td>\n",
       "      <td>1.182</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>39.00000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.261</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>3.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.223</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>44.00000</td>\n",
       "      <td>510.000000</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.222</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>8.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>32.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.443</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>39.00000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>36.5</td>\n",
       "      <td>1.057</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>7.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>41.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.391</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.258</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>1.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.197</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>6.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>35.5</td>\n",
       "      <td>0.278</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>2.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.766</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>9.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>31.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.403</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>9.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.142</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>48.00000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>31.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg   plas  pres      skin        insu  mass   pedi   age  target\n",
       "0     6.0  148.0  72.0  35.00000  155.548223  33.6  0.627  50.0       1\n",
       "1     1.0   85.0  66.0  29.00000  155.548223  26.6  0.351  31.0       0\n",
       "2     8.0  183.0  64.0  29.15342  155.548223  23.3  0.672  32.0       1\n",
       "3     1.0   89.0  66.0  23.00000   94.000000  28.1  0.167  21.0       0\n",
       "4     0.0  137.0  40.0  35.00000  168.000000  43.1  2.288  33.0       1\n",
       "5     5.0  116.0  74.0  29.15342  155.548223  25.6  0.201  30.0       0\n",
       "6     3.0   78.0  50.0  32.00000   88.000000  31.0  0.248  26.0       1\n",
       "7    10.0  115.0   0.0  29.15342  155.548223  35.3  0.134  29.0       0\n",
       "8     2.0  197.0  70.0  45.00000  543.000000  30.5  0.158  53.0       1\n",
       "9     8.0  125.0  96.0  29.15342  155.548223   0.0  0.232  54.0       1\n",
       "10    4.0  110.0  92.0  29.15342  155.548223  37.6  0.191  30.0       0\n",
       "11   10.0  168.0  74.0  29.15342  155.548223  38.0  0.537  34.0       1\n",
       "12   10.0  139.0  80.0  29.15342  155.548223  27.1  1.441  57.0       0\n",
       "13    1.0  189.0  60.0  23.00000  846.000000  30.1  0.398  59.0       1\n",
       "14    5.0  166.0  72.0  19.00000  175.000000  25.8  0.587  51.0       1\n",
       "15    7.0  100.0   0.0  29.15342  155.548223  30.0  0.484  32.0       1\n",
       "16    0.0  118.0  84.0  47.00000  230.000000  45.8  0.551  31.0       1\n",
       "17    7.0  107.0  74.0  29.15342  155.548223  29.6  0.254  31.0       1\n",
       "18    1.0  103.0  30.0  38.00000   83.000000  43.3  0.183  33.0       0\n",
       "19    1.0  115.0  70.0  30.00000   96.000000  34.6  0.529  32.0       1\n",
       "20    3.0  126.0  88.0  41.00000  235.000000  39.3  0.704  27.0       0\n",
       "21    8.0   99.0  84.0  29.15342  155.548223  35.4  0.388  50.0       0\n",
       "22    7.0  196.0  90.0  29.15342  155.548223  39.8  0.451  41.0       1\n",
       "23    9.0  119.0  80.0  35.00000  155.548223  29.0  0.263  29.0       1\n",
       "24   11.0  143.0  94.0  33.00000  146.000000  36.6  0.254  51.0       1\n",
       "25   10.0  125.0  70.0  26.00000  115.000000  31.1  0.205  41.0       1\n",
       "26    7.0  147.0  76.0  29.15342  155.548223  39.4  0.257  43.0       1\n",
       "27    1.0   97.0  66.0  15.00000  140.000000  23.2  0.487  22.0       0\n",
       "28   13.0  145.0  82.0  19.00000  110.000000  22.2  0.245  57.0       0\n",
       "29    5.0  117.0  92.0  29.15342  155.548223  34.1  0.337  38.0       0\n",
       "..    ...    ...   ...       ...         ...   ...    ...   ...     ...\n",
       "738   2.0   99.0  60.0  17.00000  160.000000  36.6  0.453  21.0       0\n",
       "739   1.0  102.0  74.0  29.15342  155.548223  39.5  0.293  42.0       1\n",
       "740  11.0  120.0  80.0  37.00000  150.000000  42.3  0.785  48.0       1\n",
       "741   3.0  102.0  44.0  20.00000   94.000000  30.8  0.400  26.0       0\n",
       "742   1.0  109.0  58.0  18.00000  116.000000  28.5  0.219  22.0       0\n",
       "743   9.0  140.0  94.0  29.15342  155.548223  32.7  0.734  45.0       1\n",
       "744  13.0  153.0  88.0  37.00000  140.000000  40.6  1.174  39.0       0\n",
       "745  12.0  100.0  84.0  33.00000  105.000000  30.0  0.488  46.0       0\n",
       "746   1.0  147.0  94.0  41.00000  155.548223  49.3  0.358  27.0       1\n",
       "747   1.0   81.0  74.0  41.00000   57.000000  46.3  1.096  32.0       0\n",
       "748   3.0  187.0  70.0  22.00000  200.000000  36.4  0.408  36.0       1\n",
       "749   6.0  162.0  62.0  29.15342  155.548223  24.3  0.178  50.0       1\n",
       "750   4.0  136.0  70.0  29.15342  155.548223  31.2  1.182  22.0       1\n",
       "751   1.0  121.0  78.0  39.00000   74.000000  39.0  0.261  28.0       0\n",
       "752   3.0  108.0  62.0  24.00000  155.548223  26.0  0.223  25.0       0\n",
       "753   0.0  181.0  88.0  44.00000  510.000000  43.3  0.222  26.0       1\n",
       "754   8.0  154.0  78.0  32.00000  155.548223  32.4  0.443  45.0       1\n",
       "755   1.0  128.0  88.0  39.00000  110.000000  36.5  1.057  37.0       1\n",
       "756   7.0  137.0  90.0  41.00000  155.548223  32.0  0.391  39.0       0\n",
       "757   0.0  123.0  72.0  29.15342  155.548223  36.3  0.258  52.0       1\n",
       "758   1.0  106.0  76.0  29.15342  155.548223  37.5  0.197  26.0       0\n",
       "759   6.0  190.0  92.0  29.15342  155.548223  35.5  0.278  66.0       1\n",
       "760   2.0   88.0  58.0  26.00000   16.000000  28.4  0.766  22.0       0\n",
       "761   9.0  170.0  74.0  31.00000  155.548223  44.0  0.403  43.0       1\n",
       "762   9.0   89.0  62.0  29.15342  155.548223  22.5  0.142  33.0       0\n",
       "763  10.0  101.0  76.0  48.00000  180.000000  32.9  0.171  63.0       0\n",
       "764   2.0  122.0  70.0  27.00000  155.548223  36.8  0.340  27.0       0\n",
       "765   5.0  121.0  72.0  23.00000  112.000000  26.2  0.245  30.0       0\n",
       "766   1.0  126.0  60.0  29.15342  155.548223  30.1  0.349  47.0       1\n",
       "767   1.0   93.0  70.0  31.00000  155.548223  30.4  0.315  23.0       0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M0uktoPhoJG_"
   },
   "source": [
    "Re-split the data and fit a new classifier.\n",
    "\n",
    "* Is performance better or worse with imputed values? Why might this be? [2] ____\n",
    "\n",
    "answer: \n",
    "\n",
    "1.   In this situation, the performance is just a little bit better than without imputed value. From k=1 to k=30, there are 7 groups whose mean accuracy become worse, and 23 groups whose mean accuracy become better. The performance depends on k value \n",
    "2.   In the original samples, the feature [skin] and [insu] miss some data. We trained the model based on this wrong data. When we test the model, we also use the data with some missing value. However, we imputed mean value for these missing data. we can consider that the imputed value is also wrong data. Therefore, this method does not improve so much\n",
    "3. The classifier uses knn to classfy the training data. KNN is based on euclidean distance.However,the data have eight features. If only one or two features miss data, it doesn't effect the classification too much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1605,
     "status": "ok",
     "timestamp": 1569443110814,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "pHMDu8jtPJbH",
    "outputId": "08888f2f-a663-4bc7-d342-dc5ba333df1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6796650717703349, 0.7122351332877648, 0.7030587833219413, 0.7187115516062884, 0.7213773069036227, 0.7357142857142858, 0.7396274777853726, 0.7383116883116883, 0.7383458646616542, 0.7434723171565277, 0.7369446343130555, 0.7473684210526316, 0.7422077922077922, 0.7539131920710869, 0.7448051948051948, 0.7526144907723855, 0.7552973342447027, 0.7552802460697198, 0.7474709501025291, 0.7461893369788107, 0.7500683526999316, 0.7501196172248804, 0.7475222146274778, 0.7435919343814081, 0.7462064251537937, 0.7331681476418319, 0.7370813397129188, 0.7305365686944635, 0.7318523581681476]\n",
      "how many groups become worse 23 how many groups become better 7\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "n_featuredata=diabetes_data.data\n",
    "n_targetdata=diabetes_data.target\n",
    "X = n_featuredata\n",
    "Y = n_targetdata\n",
    "k_range = range(1,30)\n",
    "k_scoresnew = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, Y, cv=10, scoring='accuracy')\n",
    "    k_scoresnew.append(scores.mean())\n",
    "\n",
    " #print(k_scoresnew)\n",
    "a=np.array(k_scoresnew)\n",
    "\n",
    "numbers=0\n",
    "print(k_scores)\n",
    "for k in range(0,29):\n",
    "    if k_scoresnew[k]>k_scores[k]:\n",
    "        numbers+=1\n",
    "print (\"how many groups become worse\",numbers,\"how many groups become better\",(30-numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10656,
     "status": "ok",
     "timestamp": 1569440249074,
     "user": {
      "displayName": "jianhui li",
      "photoUrl": "",
      "userId": "07548172005458139331"
     },
     "user_tz": 240
    },
    "id": "ZZmUpNiIQdZ6",
    "outputId": "08d33a45-8516-422c-dfa9-7b5ca5b83a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7579460013670541\n",
      "0.7552973342447027\n",
      "(array([17]),)\n",
      "(array([16]),)\n"
     ]
    }
   ],
   "source": [
    "a=np.array(k_scoresnew)\n",
    "b=np.array(k_scores)\n",
    "print(a.max())\n",
    "print(b.max())\n",
    "print(np.where(a == np.amax(a)))\n",
    "print(np.where(b == np.amax(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUiR--WkhZSo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
